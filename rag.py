# -*- coding: utf-8 -*-
"""rag.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/155P5SsyoVh2Jl9xg1qpvwnyenN_lxUAq
"""





"""### RAG on using this dataset -> # GPT 3.5"""

!pip install openai

#!pip install --upgrade pip

!pip install transformers torch

!pip install llama-index llama-index-experimental

import logging
import sys
from IPython.display import Markdown, display

import pandas as pd
from llama_index.experimental.query_engine import PandasQueryEngine


logging.basicConfig(stream=sys.stdout, level=logging.INFO)
logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))

from llama_index.llms.openai import OpenAI

llm = OpenAI(model="gpt-3.5-turbo", api_key="sk-proj-fPxf5ypW6BhdQP5XMgetT3BlbkFJI5hOUj2DsSwlUI5ZwN5b")

import openai
openai.api_key = 'sk-proj-fPxf5ypW6BhdQP5XMgetT3BlbkFJI5hOUj2DsSwlUI5ZwN5b'

query_engine = PandasQueryEngine(df=haa_trainChronologies,llm=llm, verbose=True)



file_formats = {
    "csv": pd.read_csv,
    "xls": pd.read_excel,
    "xlsx": pd.read_excel,
    "xlsm": pd.read_excel,
    "xlsb": pd.read_excel,
}

def load_data(uploaded_file):
    try:
        ext = os.path.splitext(uploaded_file.name)[1][1:].lower()
    except:
        ext = uploaded_file.split(".")[-1]
    if ext in file_formats:
        return file_formats[ext](uploaded_file)
    else:
        st.error(f"Unsupported file format: {ext}")
        return None

# Read the Pandas DataFrame

df = haa_develChronologies

!pip install langchain

!pip install langchain_openai

from langchain_openai.chat_models import ChatOpenAI

# Set your API key and model choice
OPENAI_API_KEY = "sk-proj-0oig5Ll0qDjkUPCGlzq8T3BlbkFJHuYBzj0UJkkUy2Wgfh3O"
MODEL_NAME = "gpt-3.5-turbo"  # Specify the model you want to use

# Initialize the ChatOpenAI model with a specific temperature
model = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=MODEL_NAME, temperature=0.5)

# Example usage
response = model.invoke("What MLB team won the World Series during the COVID-19 pandemic?")
print(response)

!pip show langchain  # Check the installed version
!pip install --upgrade langchain  # Update langchain

!pip install langchain_experimental



#from langchain.agents.agent_types import AgentType

df= haa_develChronologies
from langchain.agents.agent_types import AgentType  # Use the experimental version of AgentType
from langchain_experimental.agents import create_pandas_dataframe_agent  # Assuming you also need this experimental feature

from langchain.callbacks import StreamlitCallbackHandler
from langchain.chat_models import ChatOpenAI

llm = ChatOpenAI(temperature=0, model="gpt-3.5-turbo-0613", openai_api_key="sk-proj-0oig5Ll0qDjkUPCGlzq8T3BlbkFJHuYBzj0UJkkUy2Wgfh3O", streaming=True)
pandas_df_agent = create_pandas_dataframe_agent(llm,df,verbose=True,agent_type=AgentType.OPENAI_FUNCTIONS,handle_parsing_errors=True,)

response = pandas_df_agent.run("provide me time stemp for observations C0392747")



dataFrameResponse = pandas_df_agent.run("provide me dataframe as string")

type(dataFrameResponse)

!pip install langchain_core

!pip install ChatOpenAI

from langchain_core.output_parsers import StrOutputParser

parser = StrOutputParser()
model = llm
chain = model | parser

try:
    chain.invoke({
        "context":dataFrameResponse,
        "question":"which observations are more prone have Hospital acquired pressure injury? "
    })
except Exception as e:
    print(e)